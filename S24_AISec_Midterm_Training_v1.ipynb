{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esNWMzKrVuWc"
   },
   "source": [
    "**Revised on 3/5/2024: Changed source files**\n",
    "\n",
    "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
    "\n",
    "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
    "\n",
    "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
    "\n",
    "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
    "\n",
    "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
    "\n",
    "**Step 4:** Download the pre-processed training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUq_FZwmZegw",
    "outputId": "9998a6b9-a85f-4c62-850e-a677dde41709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-10 19:28:02--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
      "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.24.128, 52.217.205.193, 52.217.225.41, ...\n",
      "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.24.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
      "Saving to: ‘X_train.dat’\n",
      "\n",
      "100%[====================================>] 7,619,200,000 65.7MB/s   in 1m 57s \n",
      "\n",
      "2024-03-10 19:30:00 (61.9 MB/s) - ‘X_train.dat’ saved [7619200000/7619200000]\n",
      "\n",
      "--2024-03-10 19:30:00--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
      "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 3.5.1.139, 52.216.49.153, 54.231.159.17, ...\n",
      "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|3.5.1.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
      "Saving to: ‘X_test.dat’\n",
      "\n",
      "100%[====================================>] 1,904,800,000 54.6MB/s   in 31s    \n",
      "\n",
      "2024-03-10 19:30:32 (57.8 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
      "\n",
      "--2024-03-10 19:30:32--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
      "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.29.236, 52.216.27.132, 16.182.97.97, ...\n",
      "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.29.236|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3200000 (3.1M) [binary/octet-stream]\n",
      "Saving to: ‘y_train.dat’\n",
      "\n",
      "100%[======================================>] 3,200,000   --.-K/s   in 0.1s    \n",
      "\n",
      "2024-03-10 19:30:32 (23.1 MB/s) - ‘y_train.dat’ saved [3200000/3200000]\n",
      "\n",
      "--2024-03-10 19:30:32--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
      "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.144.11, 3.5.25.53, 52.217.43.60, ...\n",
      "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.144.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 800000 (781K) [binary/octet-stream]\n",
      "Saving to: ‘y_test.dat’\n",
      "\n",
      "100%[======================================>] 800,000     --.-K/s   in 0.009s  \n",
      "\n",
      "2024-03-10 19:30:33 (87.2 MB/s) - ‘y_test.dat’ saved [800000/800000]\n",
      "\n",
      "--2024-03-10 19:30:33--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
      "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.29.236, 52.216.144.11, 3.5.25.53, ...\n",
      "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.29.236|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 92160330 (88M) [text/csv]\n",
      "Saving to: ‘metadata.csv’\n",
      "\n",
      "100%[======================================>] 92,160,330  79.0MB/s   in 1.1s   \n",
      "\n",
      "2024-03-10 19:30:34 (79.0 MB/s) - ‘metadata.csv’ saved [92160330/92160330]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~8GB\n",
    "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
    "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
    "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
    "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
    "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ_JdZKfG7Q-",
    "outputId": "27a789bb-23c8-4e4b-df0a-d723e7e61bb1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V958PbDW3H0"
   },
   "source": [
    "**Step 5:** Copy the downloaded files to vMalConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vMalConv/X_train.dat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# shutil.move('X_train.dat', 'vMalConv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir vMalConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "llip77F3amma"
   },
   "outputs": [],
   "source": [
    "!mv X_train.dat vMalConv/\n",
    "!mv X_test.dat vMalConv/\n",
    "!mv y_train.dat vMalConv/\n",
    "!mv y_test.dat vMalConv/\n",
    "!mv metadata.csv vMalConv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbRilyqTXnrE"
   },
   "source": [
    "**Step 6:** Download and install Ember:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76bc7PEmlwKB",
    "outputId": "3ab8a978-cdc0-4429-d789-ffd48c047602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PFGimenez/ember.git\n",
      "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-8s_p63d7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-8s_p63d7\n",
      "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ember\n",
      "  Building wheel for ember (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=e48e89adf55279329d9fd26d8b61134afc6f9cc90d40c6e1d0c1b8b629c0af34\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-61en76qi/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
      "Successfully built ember\n",
      "Installing collected packages: ember\n",
      "Successfully installed ember-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PFGimenez/ember.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aRVMSwCQT7D",
    "outputId": "887dcff3-efa0-4102-c8e8-73dbf7ba3fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lief\n",
      "  Downloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lief\n",
      "Successfully installed lief-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.3.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.22.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.3.0-py3-none-manylinux_2_26_x86_64.whl size=2461532 sha256=15ef0629c365db9866d0815190ca463237fe821134e3400a0c526d76aeb70fd9\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6b/92/ab/b7b5df76502b64443c1a830e5f7ec3cb66741313ddebb682aa\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXym5qd8Yv8f"
   },
   "source": [
    "**Step 7:** Read vectorized features from the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfcHyoTsmCFH",
    "outputId": "bce65fa3-49da-4d55-ee08-ce4217ffc4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.13.2-2d9855fc found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n"
     ]
    }
   ],
   "source": [
    "import ember\n",
    "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"vMalConv/\")\n",
    "metadata_dataframe = ember.read_metadata(\"vMalConv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha256</th>\n",
       "      <th>appeared</th>\n",
       "      <th>label</th>\n",
       "      <th>avclass</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n",
       "      <td>2006-12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n",
       "      <td>2007-01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eac8ddb4970f8af985742973d6f0e06902d42a3684d791...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n",
       "      <td>2007-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>e033bc4967ce64bbb5cafdb234372099395185a6e0280c...</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>1</td>\n",
       "      <td>zbot</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>c7d16736fd905f5fbe4530670b1fe787eb12ee86536380...</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>1</td>\n",
       "      <td>flystudio</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>0020077cb673729209d88b603bddf56b925b18e682892a...</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>1b7e7c8febabf70d1c17fe3c7abf80f33003581c380f28...</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>836063f2312b597632bca1f738e68e4d23f672d587a7fc...</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>1</td>\n",
       "      <td>emotet</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sha256 appeared  label  \\\n",
       "0       0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  2006-12      0   \n",
       "1       c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  2007-01      0   \n",
       "2       eac8ddb4970f8af985742973d6f0e06902d42a3684d791...  2007-02      0   \n",
       "3       7f513818bcc276c531af2e641c597744da807e21cc1160...  2007-02      0   \n",
       "4       ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  2007-02      0   \n",
       "...                                                   ...      ...    ...   \n",
       "999995  e033bc4967ce64bbb5cafdb234372099395185a6e0280c...  2018-12      1   \n",
       "999996  c7d16736fd905f5fbe4530670b1fe787eb12ee86536380...  2018-12      1   \n",
       "999997  0020077cb673729209d88b603bddf56b925b18e682892a...  2018-12      0   \n",
       "999998  1b7e7c8febabf70d1c17fe3c7abf80f33003581c380f28...  2018-12      0   \n",
       "999999  836063f2312b597632bca1f738e68e4d23f672d587a7fc...  2018-12      1   \n",
       "\n",
       "          avclass subset  \n",
       "0             NaN  train  \n",
       "1             NaN  train  \n",
       "2             NaN  train  \n",
       "3             NaN  train  \n",
       "4             NaN  train  \n",
       "...           ...    ...  \n",
       "999995       zbot   test  \n",
       "999996  flystudio   test  \n",
       "999997        NaN   test  \n",
       "999998        NaN   test  \n",
       "999999     emotet   test  \n",
       "\n",
       "[1000000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTRCz7m7Z7EH"
   },
   "source": [
    "**Step 8:** Get rid of rows with no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zj63lcvin44q"
   },
   "outputs": [],
   "source": [
    "labelrows = (y_train != -1)\n",
    "X_train = X_train[labelrows]\n",
    "y_train = y_train[labelrows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVG59AGooyC5"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('X_train.h5', 'w')\n",
    "h5f.create_dataset('X_train', data=X_train)\n",
    "h5f.close()\n",
    "h5f = h5py.File('y_train.h5', 'w')\n",
    "h5f.create_dataset('y_train', data=y_train)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tmUIJNvpZch"
   },
   "outputs": [],
   "source": [
    "# !cp X_train.h5 /vMalConv/\n",
    "# !cp y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKoXSzp59RN-"
   },
   "source": [
    "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X9wwv_n9QkY"
   },
   "outputs": [],
   "source": [
    "### Your code (optional) for sampling the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1bRlBWlaQdd"
   },
   "source": [
    "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g1ZlKQwDv4uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MalConv(\n",
      "  (embed): Embedding(256, 8)\n",
      "  (filter): Conv1d(8, 128, kernel_size=(512,), stride=(512,))\n",
      "  (attention): Conv1d(8, 128, kernel_size=(512,), stride=(512,))\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MalConv(nn.Module):\n",
    "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=128, output_dim=1):\n",
    "        super(MalConv, self).__init__()\n",
    "        self.embed = nn.Embedding(256, embedding_dim)  # 256 unique bytes, embedding dimension\n",
    "        ####### Create the remaining layers Here\n",
    "        self.filter = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)\n",
    "        self.attention = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)\n",
    "        self.fc1 = nn.Linear(window_size, window_size)\n",
    "        self.fc2 = nn.Linear(window_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool1d(kernel_size=500)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x.long())\n",
    "        x = x.transpose(1, 2)  # Conv1d expects (batch_size, channels, length)\n",
    "        \n",
    "        ####### Complete this code #############\n",
    "        filt = self.filter(x)\n",
    "#         filt = self.relu(filt)\n",
    "        attn = self.attention(x)\n",
    "        attn = self.sigmoid(attn)\n",
    "        gated = filt * attn\n",
    "        x = self.maxpool(gated)\n",
    "        \n",
    "#         x = x.squeeze(2)\n",
    "#         print(gated.shape, x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Example of creating a MalConv model\n",
    "input_length = 2000000  # The fixed length for each input file\n",
    "model = MalConv(input_length=input_length)\n",
    "print(model)\n",
    "\n",
    "# Example input (a batch of byte sequences, padded or truncated to the fixed length)\n",
    "# For actual use, replace 'torch.randint' with your preprocessed byte sequence tensor\n",
    "example_input = torch.randint(0, 256, (4, 2381), dtype=torch.long)  # 4 examples, random data\n",
    "output = model(example_input)\n",
    "print(output.shape)  # The output probabilities for each example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2381])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4771],\n",
       "        [0.4853],\n",
       "        [0.4760],\n",
       "        [0.4843]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pihnLcFmbaet"
   },
   "source": [
    "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 2381)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H4q5OfK9v9iN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "mms = StandardScaler()\n",
    "for x in range(0,600000,100000):\n",
    "  mms.partial_fit(X_train[x:x+100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B33Oa1sTxdB0"
   },
   "outputs": [],
   "source": [
    "X_train = mms.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 2381)\n",
      "(2381,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V_vl5yrex0yY"
   },
   "outputs": [],
   "source": [
    "## Reshape to create 3 channels ##\n",
    "import numpy as np\n",
    "X_train = np.reshape(X_train,(-1,1,2381))\n",
    "y_train = np.reshape(y_train,(-1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1iRXFtuvCps"
   },
   "source": [
    "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ja3fhJI6qJKN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming MalConv class definition is already provided as above\n",
    "\n",
    "# Convert your numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for training and validation sets\n",
    "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
    "\n",
    "batch_size = 8     # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def byte_sequence_to_numpy(byte_sequence, max_length=input_length):\n",
    "#     # Convert byte sequence to numpy array\n",
    "#     numpy_array = np.frombuffer(byte_sequence, dtype=np.uint8)\n",
    "    \n",
    "#     # Truncate or pad the array to match the maximum length\n",
    "#     if len(numpy_array) > max_length:\n",
    "#         numpy_array = numpy_array[:max_length]\n",
    "#     else:\n",
    "#         padding_length = max_length - len(numpy_array)\n",
    "#         padding = np.zeros(padding_length, dtype=np.uint8)\n",
    "#         numpy_array = np.concatenate((numpy_array, padding))\n",
    "    \n",
    "#     return numpy_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding_char = 255\n",
    "\n",
    "# def bytez_to_numpy(arrays,maxlen=input_length):\n",
    "#     bs = np.empty(shape=(maxlen))\n",
    "#     for bytez in arrays:\n",
    "#         bytez = bytez.tobytes()\n",
    "#         b = np.ones( (maxlen,), dtype=np.uint16 )*padding_char\n",
    "#         bytez = np.frombuffer( bytez[:maxlen], dtype=np.uint8 )\n",
    "#         b[:len(bytez)] = bytez\n",
    "# #         print(bs.shape, b.shape)\n",
    "#         bs = np.vstack([bs, b])\n",
    "        \n",
    "#     return bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zMgth6McCqV"
   },
   "source": [
    "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 1, 2381)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv7piF7dp0lm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.17198535126085226\n",
      "Validation Loss: 0.12862821418930082\n",
      "Model checkpoint saved to vMalConv/model_epoch_1.pt\n",
      "Epoch 2, Training Loss: 0.11466917951138307\n",
      "Validation Loss: 0.12032758187896057\n",
      "Model checkpoint saved to vMalConv/model_epoch_2.pt\n",
      "Epoch 3, Training Loss: 0.09603821906792746\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Initialize the MalConv model\n",
    "model = MalConv()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# Directory to save model checkpoints\n",
    "save_dir = \"vMalConv/\"\n",
    "\n",
    "# Training Loop with Validation\n",
    "num_epochs = 15  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        ########## Complete this code: ##########\n",
    "        inputs = inputs.byte()\n",
    "        \n",
    "#         inputs = [array.tobytes() for array in inputs.numpy()]\n",
    "#         inputs = [bytez_to_numpy(array) for array in inputs]\n",
    "#         inputs = [torch.tensor(array.astype('float32')) for array in inputs]\n",
    "#         inputs = torch.stack(inputs)\n",
    "#         inputs = np.reshape(inputs,(batch_size, input_length))\n",
    "#         inputs = inputs.numpy()\n",
    "#         inputs = bytez_to_numpy(inputs)\n",
    "#         inputs = inputs[1:]\n",
    "#         break\n",
    "#         inputs = torch.tensor(inputs).to(torch.int64)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze())\n",
    "        loss = criterion(outputs.unsqueeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "#             inputs = [array.tobytes() for array in inputs.numpy()]\n",
    "#             inputs = [byte_sequence_to_numpy(array) for array in inputs]\n",
    "#             inputs = [torch.tensor(array) for array in inputs]\n",
    "#             inputs = torch.stack(inputs)\n",
    "#             inputs = np.reshape(inputs,(batch_size, input_length))\n",
    "\n",
    "            inputs = inputs.byte()\n",
    "    \n",
    "#             inputs = bytez_to_numpy(inputs)\n",
    "#             inputs = inputs[1:]\n",
    "#             inputs = torch.tensor(inputs).to(torch.int64)\n",
    "\n",
    "            outputs = model(inputs.squeeze())\n",
    "            loss = criterion(outputs.unsqueeze(1), labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "    checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f'Model checkpoint saved to {checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.1767809932060089\n",
      "Validation Loss: 0.13538787926559137\n",
      "Model checkpoint saved to vMalConv/model_epoch_3.pt\n",
      "Epoch 4, Training Loss: 0.11798364543332684\n",
      "Validation Loss: 0.12117938681835125\n",
      "Model checkpoint saved to vMalConv/model_epoch_4.pt\n",
      "Epoch 5, Training Loss: 0.09826269757171954\n",
      "Validation Loss: 0.12156597245816714\n",
      "Model checkpoint saved to vMalConv/model_epoch_5.pt\n",
      "Epoch 6, Training Loss: 0.08822460069890646\n",
      "Validation Loss: 0.1301822431996254\n",
      "Model checkpoint saved to vMalConv/model_epoch_6.pt\n",
      "Epoch 7, Training Loss: 0.08110414630870325\n",
      "Validation Loss: 0.1314195684100628\n",
      "Model checkpoint saved to vMalConv/model_epoch_7.pt\n",
      "Epoch 8, Training Loss: 0.07405904789271234\n",
      "Validation Loss: 0.1327621054663003\n",
      "Model checkpoint saved to vMalConv/model_epoch_8.pt\n",
      "Epoch 9, Training Loss: 0.0707812625763994\n",
      "Validation Loss: 0.16424127275045428\n",
      "Model checkpoint saved to vMalConv/model_epoch_9.pt\n",
      "Epoch 10, Training Loss: 0.06669016188833633\n",
      "Validation Loss: 0.14050454745732077\n",
      "Model checkpoint saved to vMalConv/model_epoch_10.pt\n",
      "Epoch 11, Training Loss: 0.06326528408428231\n",
      "Validation Loss: 0.17956580665959623\n",
      "Model checkpoint saved to vMalConv/model_epoch_11.pt\n",
      "Epoch 12, Training Loss: 0.06304482300018412\n",
      "Validation Loss: 0.13598500096777388\n",
      "Model checkpoint saved to vMalConv/model_epoch_12.pt\n",
      "Epoch 13, Training Loss: 0.062024606894945114\n",
      "Validation Loss: 0.16019812103914685\n",
      "Model checkpoint saved to vMalConv/model_epoch_13.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Initialize the MalConv model\n",
    "model = MalConv()\n",
    "model.state_dict(torch.load('vMalConv/model_epoch_3.pt'))\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "# Directory to save model checkpoints\n",
    "save_dir = \"vMalConv/\"\n",
    "\n",
    "# Training Loop with Validation\n",
    "num_epochs = 15  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(2, num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        ########## Complete this code: ##########\n",
    "        inputs = inputs.byte()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.squeeze())\n",
    "        loss = criterion(outputs.unsqueeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.byte()\n",
    "            outputs = model(inputs.squeeze())\n",
    "            loss = criterion(outputs.unsqueeze(1), labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "    checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f'Model checkpoint saved to {checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 2381])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obToo1WZtD4m"
   },
   "source": [
    "**Task 3:** Complete the following code to evaluate your trained model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "83wqvS9jqppe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5005\n",
      "Precision: 0.5002\n",
      "Recall: 0.9996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and DataLoader for test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model = MalConv()\n",
    "model.state_dict(torch.load('vMalConv/model_epoch_8.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Lists to store model predictions and actual labels\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_batch in test_loader:\n",
    "\n",
    "      ########## Complete this code: ##########\n",
    "        inputs = inputs.byte()\n",
    "        y_predicted = model(inputs.squeeze())\n",
    "#         predicted = y_predicted > 0.5\n",
    "        predicted = y_predicted.round()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision = precision_score(labels, predictions)\n",
    "recall = recall_score(labels, predictions)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6fLYYxps91N"
   },
   "source": [
    "**Task 4:** Comment on the results in this text box.\n",
    "\n",
    "For testing the model performance I experimented with two thresholding methods and compared two model checkpoints:\n",
    "\n",
    "| Epochs | Threshold | Accuracy | Precision | Recall |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 13 | predicted.round() | 0.5 | 0.4706 | 0.0002 |\n",
    "| 13 | predicted > 0.5 | 0.4990 | 0.4995 | 0.9961 |\n",
    "| 8 | predicted.round() | 0.5005 |  0.5002 | 0.9996 |\n",
    "| 8 | predicted > 0.5 |  0.5073 | 0.5044 | 0.8351 |\n",
    "\n",
    "The model seems to score about as good as a uniform classifier would, which is a severe underperformance for a deep learning algorithm. The model precision is not very good either but it seems to be doing well in the recall metric, meaning most actual positive samples were identified correctly. \n",
    "\n",
    "Going back to model training, validation loss is notably higher than the training loss, indicating that the model may be overfitting to the training data. Regularization may increase model performance.\n",
    "\n",
    "An important note here is that the model was trained on malware/benign classes and the test data may contain unlabeled class as well which could have affected the model performance. \n",
    "\n",
    "At the end of this experiment the 3rd model is chosen to proceed to the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

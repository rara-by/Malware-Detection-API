{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a17bf46",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61eeeafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716c78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PFGimenez/ember.git\n",
      "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-mqpxdwpz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-mqpxdwpz\n",
      "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ember\n",
      "  Building wheel for ember (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=286be4ae828c2ac9b7d017e9a5322aae354661078b5dc552dd32e11bfe1e5a69\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sm4ygnjd/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
      "Successfully built ember\n",
      "Installing collected packages: ember\n",
      "Successfully installed ember-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PFGimenez/ember.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c529be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.3.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.22.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.3.0-py3-none-manylinux_2_26_x86_64.whl size=2461521 sha256=d616afc58a6b3c97340f6737c9cee775de8c81f32911e6da1b9f4240181b5b3d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6b/92/ab/b7b5df76502b64443c1a830e5f7ec3cb66741313ddebb682aa\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057e9a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lief\n",
      "  Downloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lief\n",
      "Successfully installed lief-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b678504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ember"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51129be",
   "metadata": {},
   "source": [
    "Download two random benign PE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac5fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 11:42:26--  https://github.com/iosifache/DikeDataset/blob/main/files/benign/002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 196223 (192K) [text/plain]\n",
      "Saving to: ‘002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe.1’\n",
      "\n",
      "100%[======================================>] 196,223     --.-K/s   in 0.009s  \n",
      "\n",
      "2024-03-12 11:42:26 (19.9 MB/s) - ‘002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe.1’ saved [196223/196223]\n",
      "\n",
      "--2024-03-12 11:42:26--  https://github.com/iosifache/DikeDataset/blob/main/files/benign/02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 195897 (191K) [text/plain]\n",
      "Saving to: ‘02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe.1’\n",
      "\n",
      "100%[======================================>] 195,897     --.-K/s   in 0.01s   \n",
      "\n",
      "2024-03-12 11:42:27 (15.9 MB/s) - ‘02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe.1’ saved [195897/195897]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/iosifache/DikeDataset/blob/main/files/benign/002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe\n",
    "\n",
    "!wget https://github.com/iosifache/DikeDataset/blob/main/files/benign/02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfd9cc",
   "metadata": {},
   "source": [
    "Build the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7a12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MalConv(nn.Module):\n",
    "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=128, output_dim=1):\n",
    "        super(MalConv, self).__init__()\n",
    "        self.embed = nn.Embedding(256, embedding_dim)  # 256 unique bytes, embedding dimension\n",
    "        self.filter = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)\n",
    "        self.attention = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(window_size, window_size)\n",
    "        self.fc2 = nn.Linear(window_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool1d(kernel_size=500)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x.long())\n",
    "        x = x.transpose(1, 2)  # Conv1d expects (batch_size, channels, length)\n",
    "\n",
    "        filt = self.filter(x)\n",
    "#         filt = self.relu(filt)\n",
    "        attn = self.attention(x)\n",
    "        attn = self.sigmoid(attn)\n",
    "        gated = filt * attn\n",
    "        x = self.maxpool(gated)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92aef854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the trained model\n",
    "def load_model(model_path):\n",
    "    model = MalConv()  # Initialize model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3da8d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the PE file\n",
    "def preprocess_PE_file(file_path, input_length):\n",
    "    \n",
    "    test_file = open(file_path, 'rb').read() # read PE file\n",
    "    extract_PE = ember.PEFeatureExtractor() # initialize extractor\n",
    "    input_PE = extract_PE.feature_vector(test_file) # read feature vector\n",
    "    input_tensor = torch.tensor(input_PE)\n",
    "    byte_seq = input_tensor.byte() # get byte sequence\n",
    "    input_data = byte_seq.reshape(1, 2381) \n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40303b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify PE file using the trained model\n",
    "def classify_PE_file(file_path, model_Path, input_length):\n",
    "    # Preprocess the PE file\n",
    "    input_data = preprocess_PE_file(file_path, input_length)\n",
    "\n",
    "    # Run the preprocessed data through the trained model\n",
    "    model = load_model(model_Path) # load model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "        \n",
    "    prediction = output > 0.5\n",
    "    \n",
    "    if prediction:\n",
    "        return \"Malware\"\n",
    "    else:\n",
    "        return \"Benign\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6650",
   "metadata": {},
   "source": [
    "Perform inference on PE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f772fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.12.3- found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "Prediction: Malware\n"
     ]
    }
   ],
   "source": [
    "model_path = 'vMalConv/model_epoch_8.pt'\n",
    "file_b1 = '002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe'\n",
    "file_b2 = '02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe'\n",
    "label = 0\n",
    "input_length = 2000000\n",
    "\n",
    "# Classify the PE file\n",
    "prediction = classify_PE_file(file_b1, model_path, input_length)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8bf62",
   "metadata": {},
   "source": [
    "Upload model to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97781a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n",
    "tar.add(\"model/\", arcname=\"model\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "s3.Bucket(\"midterm-project-malconv\").upload_file('model.tar.gz', Key='model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e066b04",
   "metadata": {},
   "source": [
    "Deploy the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf18dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint pytorch-inference-2024-03-12-20-00-41-613: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m PyTorchModel(model_data\u001b[38;5;241m=\u001b[39mmodel_data,\n\u001b[1;32m     15\u001b[0m                       role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     16\u001b[0m                       entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                       source_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/code/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                       py_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpy3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                       dependencies\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/code/requirements.txt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Deploy the model to an endpoint\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.r5.xlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use the endpoint for inference\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# result = predictor.predict(input_data)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/model.py:1654\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, **kwargs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1652\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1667\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5437\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5434\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4295\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5080\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   5075\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   5076\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5077\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5078\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5079\u001b[0m         )\n\u001b[0;32m-> 5080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   5081\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5082\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5083\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5084\u001b[0m     )\n\u001b[1;32m   5085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint pytorch-inference-2024-03-12-20-00-41-613: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Specify your model location in S3\n",
    "\n",
    "model_data = 's3://midterm-project-malconv/model.tar.gz'\n",
    "\n",
    "# Create a SageMaker model\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                      role=role,\n",
    "                      entry_point='inference.py',\n",
    "                      source_dir='model/code/',\n",
    "                      py_version='py3',\n",
    "                      dependencies=['model/code/requirements.txt'])\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "predictor = model.deploy(instance_type='ml.r5.xlarge', initial_instance_count=1,\n",
    "                        wait=True,\n",
    "                        **{'timeout':180})\n",
    "\n",
    "# Use the endpoint for inference\n",
    "# result = predictor.predict(input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdc2b7",
   "metadata": {},
   "source": [
    "Classify a PE file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b1 = '002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe'\n",
    "\n",
    "result = predictor.predict(file_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9629bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

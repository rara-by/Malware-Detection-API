import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import ember
import os
import json
# import argparse

# parser = argparse.ArgumentParser()
# parser.add_argument("binaries", help="PE files to classify")
# args = parser.parse_args()

import sys
import logging

logger = logging.getLogger()
# logger.setLevel(logging.DEBUG)
# logger.addHandler(logging.StreamHandler(sys.stdout))
logger.info(':-)')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class MalConv(nn.Module):
    def __init__(self, input_length=2000000, embedding_dim=8, window_size=128, output_dim=1):
        super(MalConv, self).__init__()
        self.embed = nn.Embedding(256, embedding_dim)  # 256 unique bytes, embedding dimension
        self.filter = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)
        self.attention = nn.Conv1d(embedding_dim, window_size, kernel_size=512, stride=512, bias=True)
        
        self.fc1 = nn.Linear(window_size, window_size)
        self.fc2 = nn.Linear(window_size, 1)
        self.relu = nn.ReLU()
#         self.maxpool = nn.MaxPool1d(kernel_size=500)
        self.maxpool = nn.AdaptiveMaxPool1d(1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embed(x.long())
        x = x.transpose(1, 2)  # Conv1d expects (batch_size, channels, length)

        filt = self.filter(x)
#         filt = self.relu(filt)
        attn = self.attention(x)
        attn = self.sigmoid(attn)
        gated = filt * attn
        x = self.maxpool(gated)

        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        print("model")
        return x

    
def model_fn(model_dir):
    """
    Load the model for inference
    """
    logger.info('%(model_dir)')
    model = MalConv()  # Initialize model
    try:
        model_path = os.path.join(model_dir, 'model_epoch_8.pt')
        logger.info(model_path)
    
        model.load_state_dict(torch.load(model_path))
        
        model.to(device)
        model.eval()
        logger.info('Done loading model')
        return model

#     except Exception as e:
#         print("First attempt failed, ", e)
#     model = load_model(model_Path) # load model
#     try:
#     with open(os.path.join(model_dir, 'model_epoch_8.pt'), 'rb') as f:
#         model.load_state_dict(torch.load(f))

#         model.to(device)
#         model.eval()
#         logger.info('Done loading model')
#         return model
    
    except Exception as e:
        print("attempt failed, ", e)

def input_fn(input_data, content_type):
    """
    Deserialize and prepare the prediction input
    """

#     if request_content_type == 'application/python-pickle':
#         return torch.load(BytesIO(request_body))
    if content_type == "application/json":
#        if content_type == 'application/json':
        # Parse JSON request body
        payload = json.loads(input_data)
        payload = json.loads(payload)
#         payload = json.loads(payload)
        logging.info("PAYLOAD LOADED {}".format(type(payload))) 
        data = np.array(payload['inputs'])
        data = torch.tensor(data, dtype=torch.float32, device=device)
        
        logger.info("input processed")
        return data

#     return request


def predict_fn(input_data, model):
    """
    Apply model to the incoming request
    """

#     input_data = preprocess_PE_file(file_path, input_length)
    model.to(device)
    # Run the preprocessed data through the trained model
#     model = load_model(model_Path) # load model
    with torch.no_grad():
        output = model(input_data)
        
    prediction = output > 0.5
    logger.info("prediction processed")
    if prediction:
        return "Malware"
    else:
        return "Benign"


def output_fn(prediction, response_content_type):
    """
    Serialize and prepare the prediction output
    """
    if response_content_type == "application/json":
        response = str(prediction)
        
    logger.info("output processed")
    return json.dumps(response)
